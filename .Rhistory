<<<<<<< HEAD
# Display summaries for models 6 and 7
lapply(model_results_6_7, summary)
model_alpha <- lmer(PE ~ Group * Trial + (1 | Subject), data = long_pe_list[[7]])
# Summary of results
summary(model_alpha)
model_alpha <- lmer(PE ~ Group + Trial + Sex + (1 | Subject), data = long_pe_list[[7]])
# Summary of results
summary(model_alpha)
# Mixed-effects model to compare learning rate across trials between groups
model_alpha <- lmer(PE ~ Group * Trial * Sex + (1 | Subject), data = long_pe_list[[7]])
# Summary of results
summary(model_alpha)
model_alpha <- lmer(PE ~ Group *  Sex + (1 | Subject), data = long_pe_list[[7]])
# Summary of results
summary(model_alpha)
head(long_pe_list[[7]])
model_alpha <- lmer(PE ~ Group * Task * Sex + (1 | Subject), data = long_pe_list[[7]])
# Summary of results
summary(model_alpha)
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(1:3,6,7)] function(df) {
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
=======
library(ggseg)
library(ggseg) ; library(ggsegGlasser) ; library(rethinking); library(tidyverse)
someData <- tibble(
region = rep(c("transverse temporal", "insula",
"precentral","superior parietal"), 2),
p = sample(seq(0,.5,.001), 8),
groups = c(rep("g1", 4), rep("g2", 4))
)
someData %>%
group_by(groups) %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = p)) +
facet_wrap(~groups)
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R", "R")
)
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R")
)
p1 <- group12 %>%
group_by(hemisphere)  %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = t)) +
ggtitle("Iyi Yanit > Kotu Yanit")+
mytheme +
scale_fill_gradient(low = "gray", high="black", na.value = "white",limits=c(2.2,4.5))+
facet_wrap(~hemisphere)
mytheme <- theme_classic()+
theme(legend.position = "bottom",
legend.text = element_text(size = 10),
axis.line = element_blank(),
axis.ticks.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(size = 20, face = "bold", hjust = 0.5))
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R")
)
p1 <- group12 %>%
group_by(hemisphere)  %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = t)) +
ggtitle("Iyi Yanit > Kotu Yanit")+
mytheme +
scale_fill_gradient(low = "gray", high="black", na.value = "white",limits=c(2.2,4.5))+
facet_wrap(~hemisphere)
p1
ggarrange(p1,ncol = 1, common.legend = TRUE
)%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
library(ggpubr)
ggarrange(p1,ncol = 1, common.legend = TRUE
)%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
p1%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
key %>% filter(BCNatlas=="120")
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
# source_path = "C:/Users/kaan/Documents/MultGroup_WC"
source_path = "C:/Users/kaank/OneDrive/Belgeler/GitHub/MultGroup_WC"
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
library(tidyverse)
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
#### Testing if regions are belonged to core or periphery ####
transmodal <- ifelse(NeuroMyelFC::trans_nonself | NeuroMyelFC::trans_self,1,0)
key <- key %>%
mutate(Category = transmodal)
# Check consistency for each BCNatlas group
consistency_check <- key %>%
group_by(BCNatlas) %>%
summarize(
Is_Consistent = if (n_distinct(Category) == 1) "YES" else "NO" # TRUE if all categories are the same
)
# View results
print(consistency_check)
key %>% filter(BCNatlas=="120")
key %>% filter(BCNatlas=="121")
key
#### Organizing the connectivity matrix
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity.csv",row.names = 1)
#### Organizing the connectivity matrix
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
header = TRUE,
row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
subject_list
key
averaged_rois_list
Cmat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
Cmat
# Normalize by the maximum value
Cmat_max <- Cmat / max(Cmat)
# Check the normalized matrix
print(Cmat_max)
# View results for the first vector
print(averaged_rois_list[[1]])
read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
### ROI normalization ###
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roi_list
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roilist
roilist
roilist
roilist[1]
roilist[1,1]
ro
roilist[612:617,]
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roilist
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = FALSE,
row.names = 1)
roilist
roilist[612,]
roilist[1,]
roilist[612,]
# List of regions to select
regions <- c("R. transverse temporal gyrus", "R. superior temporal gyrus (gm)")
# Filter rows based on the region names in column V2
selected_voxels <- roilist[roilist$V2 %in% regions, "V3"]
# Display the result
print(selected_voxels)
# List of regions to select
regions <- c("R. transverse temporal gyrus (gm)", "R. superior temporal gyrus (gm)")
# Filter rows based on the region names in column V2
selected_voxels <- roilist[roilist$V2 %in% regions, "V3"]
# Display the result
print(selected_voxels)
key
averaged_rois_list
print(averaged_rois_list[[1]],n=59)
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
print(averaged_rois_list[[1]],n=58)
library(ggseg)
library(ggsegGlasser)
glasser
ggplot() + geom_brain()
ggsegGlasser::glasser
ggplot() + geom_brain( atlas = glasser)
ggplot() + geom_brain( atlas = glasser,
show.legend = FALSE,
position=position_brain(position= hemi ~ side),
mapping = aes(fill = "6d") )
ggplot() + geom_brain( atlas = glasser)
library(reticulate)
library(NeuroMyelFC)
# Set the Python path (adjust the path to your Python executable)
# Use the specific Python executable
#use_python("C:/Users/kaank/AppData/Local/Programs/Python/Python311/python.exe")
source_folder = "C:/Users/kaank/OneDrive/Belgeler/GitHub/MultGroup_WC/"
Sys.setenv(RETICULATE_PYTHON = "C:/Users/kaank/miniconda3/envs/multgroup/python.exe")
library(reticulate)
np <- import("numpy")
array <- np$load(paste(source_folder,"group_freekgl3.npy",sep=""),allow_pickle = TRUE)
merged_matrices <- list()
# Get the names from the list
array_names <- names(array[[1]])
# Loop through each name in the list
for (name in array_names) {
# Initialize empty matrices to store the merged results for each name
merged_acw_static_firing_matrix <- NULL
merged_acw_dynamic_firing_matrix <- NULL
merged_acw_static_fmri_matrix <- NULL
merged_gscorr_fmri_matrix <- NULL
merged_gscorr_firing_matrix <- NULL
# Loop through each element in the sublist
for (i in 1:length(array[[1]]$MDD)) {
merged_acw_static_firing_matrix <- rbind(merged_acw_static_firing_matrix, array[[1]][[name]][[i]]$acw_static_firing_matrix)
merged_acw_dynamic_firing_matrix <- rbind(merged_acw_dynamic_firing_matrix, array[[1]][[name]][[i]]$acw_dynamic_firing_matrix)
merged_acw_static_fmri_matrix <- rbind(merged_acw_static_fmri_matrix, array[[1]][[name]][[i]]$acw_static_fmri_matrix)
merged_gscorr_fmri_matrix <- rbind(merged_gscorr_fmri_matrix, array[[1]][[name]][[i]]$gscorr_fmri_matrix)
merged_gscorr_firing_matrix <- rbind(merged_gscorr_firing_matrix, array[[1]][[name]][[i]]$gscorr_firing_matrix)
}
# Save the merged matrices into the final list
merged_matrices[[name]] <- list(
merged_acw_static_firing_matrix = merged_acw_static_firing_matrix,
merged_acw_dynamic_firing_matrix = merged_acw_dynamic_firing_matrix,
merged_acw_static_fmri_matrix = merged_acw_static_fmri_matrix,
merged_gscorr_fmri_matrix = merged_gscorr_fmri_matrix,
merged_gscorr_firing_matrix = merged_gscorr_firing_matrix
)
}
gs_sim = list()
gs_sim$mdd = merged_matrices$MDD$merged_gscorr_fmri_matrix
gs_sim$hc = merged_matrices$HC$merged_gscorr_fmri_matrix
gs_sim$manic = merged_matrices$Manic$merged_gscorr_fmri_matrix
gs_sim$bp = merged_matrices$BP$merged_gscorr_fmri_matrix
uni <- NeuroMyelFC::uni_nonself | NeuroMyelFC::uni_self
trans <- NeuroMyelFC::trans_nonself | NeuroMyelFC::trans_self
gs_sim
dat <- lapply(gs_sim, function(mat){
masked_mat <- mat[, uni, drop = FALSE]  # Apply the mask to select columns
rowMeans(masked_mat)                    # Compute row-wise means for selected columns
>>>>>>> b28b3f1f9611c7e17c9e7a5b2be9eafeee027c20
})
# Display summaries for all models
lapply(model_results, summary)
# Apply the mixed-effects model to list elements 4-5
model_results <- lapply(long_pe_list[4:5], function(df) {
lm(PE ~ Group * Task * Sex , data = df)
})
# Display summaries for all models
lapply(model_results, summary)
dat <- list(readMat("./data/processed/pe_array2.mat"),
readMat("./data/processed/x2_array.mat"),
readMat("./data/processed/x3_array.mat"),
readMat("./data/processed/x2_pe_array.mat"),
readMat("./data/processed/x3_pe_array.mat"),
readMat("./data/processed/alfa2_array.mat"),
readMat("./data/processed/alfa3_array.mat"),
readMat("./data/processed/rw_pe.mat"))
# Usage
long_pe_list <- lapply(dat, convert_to_long, subj_table = subj_table)
long_pe_list[[8]]
model_results <- lapply(long_pe_list[[8]], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
long_pe_list
long_pe_list[[9]]
long_pe_list[[8]]
model_result <- lmer(PE ~ Group * Task * Sex + (1 | Subject), data = long_pe_list[[8]])
model_result <- lm(PE ~ Group * Task * Sex, data = long_pe_list[[8]])
summary(model_result)
# Apply the mixed-effects model to list elements 4-5
model_results <- lapply(long_pe_list[c(4:5,8)], function(df) {
lm(PE ~ Group * Task * Sex , data = df)
})
# Display summaries for all models
lapply(model_results, summary)
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
library(ggplot2)
ggplot(long_pe_list[[4]], aes(x = Task, y = PE, color = Group)) +
geom_smooth(method = "loess") +
facet_wrap(~ Sex) +
labs(title = "Learning Rate (α) Across Tasks", x = "Task", y = "Learning Rate (α)") +
theme_minimal()
<<<<<<< HEAD
subj_table$task.id
T_raw <- read.csv("./data/raw/response.csv")
T <- T_raw[T_raw$denekId %in% subj_table$task.id,]
T
T_aslihan <- read.csv("./data/processed/aslihan_filtered.csv")
T_aslihan
cbind(T,T_aslihan)
rbind(T,T_aslihan)
T <- rbind(T,T_aslihan)
=======
array1<-c(39,33,14,32,10,50,47,30,34,50,37,42,47,44,50,33,46,28,46,29,49,33,54,31,37,35
)
array2 <- c(48,34,51,35,25,25,46,22,8,38,52,28,10,37,27,24)
kruskal.test(array1,array2)
shapiro.test(array1)
shapiro.test(array2)
t_test_result <- t.test(array1, array2, var.equal = FALSE)
print(t_test_result)
# Loading libraries
library(R.matlab); library(tidyverse)
setwd("/Users/kaankeskin/projects/sch_pe/")
library(R.matlab); library(tidyverse)
#setwd("/Users/kaankeskin/projects/sch_pe/")
# Microsoft
setwd("C:/Users/kaank/OneDrive/Belgeler/GitHub/sch_pe/")
#
dat <- readMat("./data/processed/pe_array2.mat")
subj_table <- read.csv("./data/raw/subjects_list.csv")
subj_table <- subset(subj_table, !(subj %in% c(9, 44)))
T_raw <- read.csv("./data/raw/response.csv")
T <- T_raw[T_raw$denekId %in% subj_table$task.id,]
# Just for control. Shoul equal to 51
#sum(T$sayac==59)
>>>>>>> b28b3f1f9611c7e17c9e7a5b2be9eafeee027c20
T_last <- T[,c(2,3,4,5)]
T_last
subj_table
T_last$group <- rep(subj_table$group,each=60)
T_last$phase <- cut(
T_last$sayac,
breaks = c(0,19,39,59),
labels = c("1", "2", "3"),
include.lowest = TRUE
)
<<<<<<< HEAD
T_last
=======
library(lme4)
>>>>>>> b28b3f1f9611c7e17c9e7a5b2be9eafeee027c20
T_last$yatirim <- factor(T_last$yatirim)
T_last$rakip <- factor(T_last$rakip)
T_last$group <- factor(T_last$group)
T_last$phase <- factor(T_last$phase)
<<<<<<< HEAD
model <- glmer(
yatirim ~ group * phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit")
)
summary(model)
model <- glm(
yatirim ~ group * phase ,
data = T_last,
family = binomial(link = "logit")
)
summary(model)
=======
>>>>>>> b28b3f1f9611c7e17c9e7a5b2be9eafeee027c20
# Create a 2x2 table for each group
tables <- by(T_last, T_last$group, function(sub_df) table(sub_df$yatirim, sub_df$rakip))
tables
<<<<<<< HEAD
str(T_last)
# Create contingency table for group differences in yatırım
yatirim_table <- matrix(c(1030, 1458, 781, 808), nrow = 2, byrow = TRUE)
# Add row & column names for readability
rownames(yatirim_table) <- c("Group 0 (HC)", "Group 1 (SZ)")
colnames(yatirim_table) <- c("yatirim = 0", "yatirim = 1")
# Print table
print(yatirim_table)
# Perform Chi-Square Test
chisq_test <- chisq.test(yatirim_table)
print(chisq_test)
group
sum(group==hc)
sum(group=="hc")
n_hc <- 51
n_sz <- 31
# Compute investment rates per subject
prop_hc <- yatirim_table[1, 2] / n_hc  # HC: Number of 1s / HC subjects
prop_sz <- yatirim_table[2, 2] / n_sz  # SZ: Number of 1s / SZ subjects
# Total investments (yatırım = 1) in each group
yatirim_counts <- c(yatirim_table[1, 2], yatirim_table[2, 2])  # Count of 1s
group_sizes <- c(n_hc, n_sz)  # Number of subjects in each group
# Perform a proportion test
prop_test <- prop.test(yatirim_counts, group_sizes)
prop_hc
prop_sz
yatirim_counts <- c(yatirim_table[1, 2], yatirim_table[2, 2])  # Count of 1s
group_sizes <- c(n_hc, n_sz)  # Number of subjects in each group
yatirim_counts
# Perform a proportion test
prop_test <- prop.test(yatirim_counts, group_sizes)
# Total number of trials per group
trials_hc <- sum(yatirim_table[1, ])  # Total trials for HC
trials_sz <- sum(yatirim_table[2, ])  # Total trials for SZ
# Corrected proportion test
prop_test <- prop.test(yatirim_counts, c(trials_hc, trials_sz))
# Print results
print(prop_test)
library(lme4)
model_glmer <- glmer(
yatirim ~ group * phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit")
)
summary(model_glmer)
model_simple <- glmer(yatirim ~ group + phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit"))
anova(model_glmer, model_simple)  # Compare models
model_simple
model <- glmer(
yatirim ~ group * phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit")
)
model
model_glmer <- glmer(
yatirim ~ group * phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit")
)
model_simple <- glmer(yatirim ~ group + phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit"))
anova(model_glmer, model_simple)  # Compare models
model_simple <- glmer(yatirim ~ group + phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit"))
summary(model_simple)
str(long_pe_list[[1]])
Q1 <- quantile(long_pe_list[[1]]$PE, 0.25, na.rm = TRUE)  # 25th percentile
Q3 <- quantile(long_pe_list[[1]]$PE, 0.75, na.rm = TRUE)  # 75th percentile
IQR_value <- Q3 - Q1  # Interquartile range
# Define lower bound for outliers
lower_bound <- Q1 - 1.5 * IQR_value
# Identify outlier subjects
outlier_subjects <- long_pe_list[[1]] %>%
group_by(Subject) %>%
summarise(mean_PE = mean(PE, na.rm = TRUE)) %>%
filter(mean_PE < lower_bound)
# Print outlier subjects
print(outlier_subjects)
long_pe_list[[1]] %>%
group_by(Subject)
long_pe_list[[1]] %>%
group_by(Subject) %>%
summarise(mean_PE = mean(PE, na.rm = TRUE))
Q1 <- quantile(long_pe_list[[1]]$PE, 0.25, na.rm = TRUE)  # 25th percentile
Q3 <- quantile(long_pe_list[[1]]$PE, 0.75, na.rm = TRUE)  # 75th percentile
IQR_value <- Q3 - Q1  # Interquartile range
# Define lower bound for outliers
lower_bound <- Q1 - 1.5 * IQR_value
lower_bound
Q1
Q2
Q3
Q1 <- quantile(long_pe_list[[1]]$PE, 0.25, na.rm = TRUE)  # 25th percentile
Q3 <- quantile(long_pe_list[[1]]$PE, 0.75, na.rm = TRUE)  # 75th percentile
IQR_value <- Q3 - Q1  # Interquartile range
Q1 - 1.5 * IQR_value
lower_bound
library(dplyr)
# Compute IQR thresholds for PE
Q1 <- quantile(long_pe_list[[1]]$PE, 0.25, na.rm = TRUE)  # 25th percentile
Q3 <- quantile(long_pe_list[[1]]$PE, 0.75, na.rm = TRUE)  # 75th percentile
IQR_value <- Q3 - Q1  # Interquartile range
# Define lower and upper bounds for outliers
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value
# Find all outliers in PE
outliers <- long_pe_list[[1]] %>%
filter(PE < lower_bound | PE > upper_bound) %>%
arrange(Subject)
outliers
print(outliers)
plot(density(long_pe_list[[1]]$PE))
plot(density(long_pe_list[[4]]$PE))
plot(density(long_pe_list[[5]]$PE))
# Extract residuals from one of the models
residuals <- residuals(model_results[[1]])
# Check normality with histogram
hist(residuals, breaks = 30, main = "Residuals of LMM", xlab = "Residuals")
# QQ plot
qqnorm(residuals)
qqline(residuals, col = "red")
# Shapiro-Wilk normality test
shapiro.test(residuals)
library(lme4)
model_results <- glmer(PE ~ Group * Task * Sex + (1 | Subject), data = long_pe_list[[1]], family = Gamma(link = "log"))
log_PE <- log(long_pe_list[[1]] + 1)
log_PE <- log(long_pe_list[[1]]$PE + 1)
log_PE
long_pe_list[[1]]$log_PE <- log(long_pe_list[[1]]$PE + 1)
library(lme4)
model_results <- lmer(log_PE ~ Group * Task * Sex + (1 | Subject), data = long_pe_list[[1]])
summary(model_result)
summary(model_results)
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
model_results[[1]]
long_pe_list[[1]]$log_PE <- log(long_pe_list[[1]]$PE + 1)
library(lme4)
model_result <- lmer(log_PE ~ Group * Task * Sex + (1 | Subject), data = long_pe_list[[1]])
model_result
summary(model_results()
summary(model_results)
summary(model_results)
summary(model_result)
plot(density(long_pe_list[[1]]$log_PE))
residuals_log <- residuals(model_result)
shapiro.test(residuals_log)
min(long_pe_list[[1]]$PE)
min_PE <- min(long_pe_list[[1]]$PE)  # Find minimum value in PE
shift_value <- ifelse(min_PE <= 0, abs(min_PE) + 1e-6, 0)  # Ensure all values are > 0
shift_value
min_PE <- min(long_pe_list[[1]]$PE)  # Find minimum value in PE
shift_value <- ifelse(min_PE <= 0, abs(min_PE) + 1e-6, 0)  # Ensure all values are > 0
long_pe_list[[1]]$PE_shifted <- long_pe_list[[1]]$PE + shift_value  # Apply the shift
long_pe_list[[1]]$PE_shifted <- long_pe_list[[1]]$PE + shift_value  # Apply the shift
library(lme4)
gamma_model <- glmer(PE_shifted ~ Group * Task * Sex + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"))
gamma_model <- glmer(PE_shifted ~ Group * Task * Sex + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "Nelder_Mead"))
gamma_model <- glmer(PE_shifted ~ Group * Task * Sex + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
long_pe_list[[1]]$PE_shifted <- long_pe_list[[1]]$PE + abs(min(long_pe_list[[1]]$PE)) + 0.01
gamma_model <- glmer(PE_shifted ~ Group * Task * Sex + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
gamma_model <- glmer(PE_shifted ~ Group + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
gamma_model <- glmer(PE_shifted ~ Group * Phase + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
str(long_pe_list[[1]])
gamma_model <- glmer(PE_shifted ~ Group * Task + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
str(long_pe_list[[1]])
library(car)
vif(gamma_model)  # If VIF > 5-10, consider centering predictors
gamma_model <- glmer(PE_shifted ~ Group + Task + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
lognorm_model <- glmer(PE_shifted ~ Group * Task + (1 | Subject),
data = long_pe_list[[1]],
family = gaussian(link = "log"))
anova(gamma_model, lognorm_model)  # Compare AIC
gamma_model_c <- glmer(PE_shifted ~ Group * Task_c + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"))
long_pe_list[[1]]$Task_c <- as.numeric(long_pe_list[[1]]$Task) - mean(as.numeric(long_pe_list[[1]]$Task))
long_pe_list[[1]]$Task_c
gamma_model_c <- glmer(PE_shifted ~ Group * Task_c + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"))
summary(gamma_model_c)
library(emmeans)
emm <- emmeans(gamma_model_c, ~ Group * Task_c)
plot(emm)
gamma_model_simple <- glmer(PE_shifted ~ Group + Task_c + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"))
anova(gamma_model_simple, gamma_model_c)
model_simple <- glmer(yatirim ~ group + phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit"))
summary(model_simple)
model_int <- glmer(yatirim ~ group * phase + (1 | denekId), data = T_last, family = binomial(link = "logit"))
anova(model_simple, model_int)
exp(fixef(model_simple))  # Computes odds ratios
library(dplyr)
# Summarize investment behavior for each subject in each phase
subject_phase_summary <- T_last %>%
group_by(Subject, Group, Phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE)) %>%
pivot_wider(names_from = Phase, values_from = mean_yatirim, names_prefix = "Phase")
T_last %>%
group_by(Subject, Group, Phase)
T_last
subject_phase_summary <- T_last %>%
group_by(denekID, group, phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE)) %>%
pivot_wider(names_from = Phase, values_from = mean_yatirim, names_prefix = "Phase")
subject_phase_summary <- T_last %>%
group_by(denekId, group, phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE)) %>%
pivot_wider(names_from = Phase, values_from = mean_yatirim, names_prefix = "Phase")
subject_phase_summary <- T_last %>%
group_by(denekId, group, phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE)) %>%
pivot_wider(names_from = phase, values_from = mean_yatirim, names_prefix = "phase")
T_last %>%
group_by(denekId, group, phase)
T_last %>%
group_by(denekId, group, phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE))
str(T_last)
T_last$yatirim <- as.numeric(as.character(T_last$yatirim))
subject_phase_summary <- T_last %>%
group_by(denekId, group, phase) %>%
summarise(mean_yatirim = mean(yatirim, na.rm = TRUE)) %>%
pivot_wider(names_from = phase, values_from = mean_yatirim, names_prefix = "phase")
subject_phase_summary
subject_phase_summary <- subject_phase_summary %>%
mutate(Change_P2_P3 = Phase3 - Phase2)
subject_phase_summary <- subject_phase_summary %>%
mutate(Change_P2_P3 = phase3 - phase2)
subject_phase_summary
# Print the summary
print(subject_phase_summary)
# Compare mean investment change by group
group_change_summary <- subject_phase_summary %>%
group_by(group) %>%
summarise(mean_change = mean(Change_P2_P3, na.rm = TRUE),
sd_change = sd(Change_P2_P3, na.rm = TRUE))
print(group_change_summary)
model_simple <- glmer(yatirim ~ group + phase + (1 | denekId),
data = T_last,
family = binomial(link = "logit"))
anova(model_glmer, model_simple)  # Compare models
summary(model_simple)
gamma_model <- glmer(PE_shifted ~ Group + Task + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
model_results[[1]]
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
dat <- list(readMat("./data/processed/pe_array2.mat"), # Cemre RW PE
readMat("./data/processed/x2_array.mat"), # HGF X2
readMat("./data/processed/x3_array.mat"), # HGF X3
readMat("./data/processed/x2_pe_array.mat"), # HGF low level PE
readMat("./data/processed/x3_pe_array.mat"), # HGF high level PE
readMat("./data/processed/alfa2_array.mat"), # learning rate level 2
readMat("./data/processed/alfa3_array.mat"), # learning rate level 3
readMat("./data/processed/rw_pe.mat")) # RW model PE from TAPAS
subj_table <- read.csv("./data/raw/subjects_list.csv")
convert_to_long <- function(dat, subj_table) {
# Extract PE matrix (first 60 columns) and group information (column 61)
pe_mat <- dat$merged.matrix[,1:60]
group <- factor(ifelse(dat$merged.matrix[,61], "sz", "hc"))
task <- factor(rep(1:3, each = 20))  # Assuming 3 tasks with 20 trials each
sex <- factor(ifelse(subj_table$sex, "M", "F"))
# Convert PE matrix to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- seq_len(nrow(pe_mat))  # Assign unique IDs to subjects
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Convert Trial variable to numeric
#long_pe$Trial <- as.numeric(gsub("V", "", long_pe$Trial))  # Remove "V" prefix if needed
# Add Group information (repeat for each trial)
long_pe$Group <- rep(group, each = ncol(pe_mat))
# Add Task information (repeat for each subject)
long_pe$Task <- rep(task, times = nrow(pe_mat))
# Add Sex information (repeat for each trial)
long_pe$Sex <- rep(sex, each = ncol(pe_mat))
return(long_pe)
}
# Usage
long_pe_list <- lapply(dat, convert_to_long, subj_table = subj_table)
library(lme4)
library(lmerTest)
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(1:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
model_result <- lmer(PE ~ Group * Task * Sex + (1 + Task | Subject), data = long_pe_list[[1]])
summary(model_result)
gamma_model <- glmer(PE_shifted ~ Group + Task + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
long_pe_list[[1]]$PE_shifted <- long_pe_list[[1]]$PE + abs(min(long_pe_list[[1]]$PE)) + 0.01
library(lme4)
gamma_model <- glmer(PE_shifted ~ Group + Task + (1 | Subject),
data = long_pe_list[[1]],
family = Gamma(link = "log"),
control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 1e5)))
summary(gamma_model)
# Apply the mixed-effects model to list elements 1-5
model_results <- lapply(long_pe_list[c(2:3,6,7)], function(df) {
lmer(PE ~ Group * Task * Sex + (1 | Subject), data = df)
})
# Display summaries for all models
lapply(model_results, summary)
# Apply the mixed-effects model to list elements 4-5
model_results <- lapply(long_pe_list[c(4:5,8)], function(df) {
lm(PE ~ Group * Task * Sex , data = df)
})
# Display summaries for all models
lapply(model_results, summary)
plot(density(long_pe_list[[4]]))
plot(density(long_pe_list[[4]]$PE))
shapiro.test(long_pe_list[[4]]$PE)
shapiro.test(log(long_pe_list[[4]]$PE+1e-6))
# Define a small positive constant to avoid log(0)
epsilon <- min(long_pe_list[[4]]$PE[long_pe_list[[4]]$PE > 0], na.rm = TRUE) / 10
# Log-transform the PE values
long_pe_list[[4]]$log_PE <- log(long_pe_list[[4]]$PE + epsilon)
# Visualize the new distribution
hist(long_pe_list[[4]]$log_PE, breaks = 50, main = "Histogram of Log-Transformed PE", col = "skyblue")
plot(density(long_pe_list[[4]]$log_PE), main = "Density of Log-Transformed PE")
epsilon
shift_value <- abs(min(long_pe_list[[4]]$PE, na.rm = TRUE)) + 0.01
long_pe_list[[4]]$log_PE <- log(long_pe_list[[4]]$PE + shift_value)
hist(long_pe_list[[4]]$log_PE, breaks = 50, main = "Histogram of Log-Transformed PE", col = "skyblue")
plot(density(long_pe_list[[4]]$log_PE, na.rm = TRUE), main = "Density of Log-Transformed PE")
library(MASS)
fit_gamma <- fitdistr(long_pe_list[[4]]$PE, "gamma")
library(MASS)
fit_gamma <- fitdistr(long_pe_list[[4]]$log_PE, "gamma")
fit_lognorm <- fitdistr(long_pe_list[[4]]$PE + abs(min(long_pe_list[[4]]$PE)) + 1, "lognormal")
print(fit_lognorm)
min_val <- min(long_pe_list[[4]]$log_PE, na.rm = TRUE)
log_PE_shifted <- long_pe_list[[4]]$log_PE - min_val + 1
fit_gamma <- fitdistr(log_PE_shifted, "gamma")
print(fit_gamma)
plot(density(long_pe_list[[4]]$PE))
min(long_pe_list[[4]]$PE)
max(long_pe_list[[4]]$PE)
library(VGAM)
install.packages("VGAM")
library(VGAM)
library(VGAM)
fit_laplace <- vglm(long_pe_list[[4]]$PE ~ 1, laplace())
summary(fit_laplace)
library(MASS)
fit_t <- fitdistr(long_pe_list[[4]]$PE, "t")
summary(fit_t)
=======
# Logistic regression model
model <- glm(yatirim ~ group * rakip, data = T_last, family = binomial)
# Display model summary
summary(model)
model <- glm(
yatirim ~ group * phase * rakip,  # Adds main effects & all interactions
data = T_last,
family = binomial(link = "logit")
)
summary(model)
model <- glm(
yatirim ~ group * phase ,
data = T_last,
family = binomial(link = "logit")
)
summary(model)
model_simpler <- glm(
yatirim ~ group + phase + rakip + group:rakip,  # Focuses on group × rakip interaction
data = T_last,
family = binomial(link = "logit")
)
summary(model_simpler)
T_last$yatirim
T_last$rakip
# Create a 2x2 table for each group
tables <- by(T_last, T_last$group, function(sub_df) table(sub_df$yatirim, sub_df$rakip))
# Display tables
tables
help(table)
# Create a 2x2 table for each group
tables <- by(T_last, T_last$group, function(sub_df) table(yatirim = sub_df$yatirim, rakip = sub_df$rakip))
# Display tables
tables
# Create dummy variables
T_last$yatirim1_rakip1 <- ifelse(T_last$yatirim == 1 & T_last$rakip == 1, 1, 0)
T_last$yatirim1_rakip0 <- ifelse(T_last$yatirim == 1 & T_last$rakip == 0, 1, 0)
T_last$yatirim0_rakip1 <- ifelse(T_last$yatirim == 0 & T_last$rakip == 1, 1, 0)
model_dummies <- glm(
yatirim ~ group * phase + yatirim1_rakip1 + yatirim1_rakip0 + yatirim0_rakip1,
data = T_last,
family = binomial(link = "logit")
)
summary(model_dummies)
T_last$yatirim == 0 & T_last$rakip == 1
T_last %>% filter(yatirim == 0)
model_dummies <- glm(
yatirim ~ group * phase + yatirim1_rakip1 + yatirim1_rakip0 + yatirim0_rakip1,
data = T_last,
family = binomial(link = "logit")
)
summary(model_dummies)
T_last <- T_last %>%
arrange(subject, trial) %>%  # Ensure data is ordered by subject and trial
group_by(subject) %>%
mutate(cum_rakip = cumsum(lag(rakip, default = 0))) %>%  # Cumulative sum of previous rakip values
ungroup()
head(T_last)
T_last <- T_last %>%
arrange(denekId, sayac) %>%  # Ensure data is ordered by subject and trial
group_by(denekID) %>%
mutate(cum_rakip = cumsum(lag(rakip, default = 0))) %>%  # Cumulative sum of previous rakip values
ungroup()
T_last
T_last %>%
arrange(denekId, sayac)
T_last %>%
arrange(denekId, sayac) %>%  # Ensure data is ordered by subject and trial
group_by(denekID)
T_last <- T_last %>%
arrange(denekId, sayac) %>%  # Ensure data is ordered by subject and trial
group_by(denekId) %>%
mutate(cum_rakip = cumsum(lag(rakip, default = 0))) %>%  # Cumulative sum of previous rakip values
ungroup()
T_last <- T_last %>%
arrange(denekId, sayac) %>%  # Ensure data is ordered by subject and trial
group_by(denekId) %>%  # Group by subject
mutate(cum_rakip = cumsum(lag(as.numeric(as.character(rakip)), default = 0))) %>%  # Convert rakip to numeric
ungroup()
T_last
T_last$cum_rakip
head(T_last)
T_raw <- read.csv("./data/raw/response.csv")
T <- T_raw[T_raw$denekId %in% subj_table$task.id,]
# Just for control. Shoul equal to 51
#sum(T$sayac==59)
T_last <- T[,c(2,3,4,5)]
T_last$group <- rep(subj_table$group,each=60)
T_last$phase <- cut(
T_last$sayac,
breaks = c(0,19,39,59),
labels = c("1", "2", "3"),
include.lowest = TRUE
)
T_last$yatirim <- factor(T_last$yatirim)
T_last$group <- factor(T_last$group)
T_last$phase <- factor(T_last$phase)
T_last <- T_last %>%
arrange(denekId, sayac) %>%  # Ensure data is ordered by subject and trial
group_by(denekId) %>%
mutate(cum_rakip = cumsum(lag(rakip, default = 0))) %>%  # Cumulative sum of previous rakip values
ungroup()
T_last
T_last %>% filter(denekId==8)
View(T_last %>% filter(denekId==8))
model_cum_rakip <- glm(
yatirim ~ group * phase + cum_rakip + group:cum_rakip,
data = T_last,
family = binomial(link = "logit")
)
summary(model_cum_rakip)
>>>>>>> b28b3f1f9611c7e17c9e7a5b2be9eafeee027c20
