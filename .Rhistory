library(MASS)
# Category-level random intercept & slope for BDI
RE_gs_cat <- mvrnorm(K, mu=c(0,0), Sigma=Sigma_gs)
cat_int_offset  <- RE_gs_cat[,1]
cat_slope_offset<- RE_gs_cat[,2]
n_regions <- 5
n_obs     <- n_subj * n_regions
subj_idx  <- rep(1:n_subj, each=n_regions)
cat_idx   <- sample.int(K, size=n_obs, replace=TRUE)  # e.g., 1..K uniformly
GSCORR_vec <- numeric(n_obs)
for(i in 1:n_obs) {
s <- subj_idx[i]      # subject for obs i
k <- cat_idx[i]       # category for obs i
mu_gs_i <- (gamma0_true + cat_int_offset[k]) +
betaA_true * Age_vec[s] +
betaS_true * Sex_vec[s] +
betaM_true * Med_vec[s] +
betaG_true * Group_vec[s] +
(betaB_true + cat_slope_offset[k]) * BDI_vec[s]
GSCORR_vec[i] <- rnorm(1, mu_gs_i, sigma_GSCORR_true)
}
stan_data <- list(
# Indices & sizes
n = n_obs,
n_subj = n_subj,
n_regions = n_regions,
K = K,
# Subject-level covariates
S = as.integer(Sex_vec),
A = as.vector(Age_vec),
G = as.vector(Group_vec),
M = as.vector(Med_vec),
# Subject-level outcome
BDI = as.vector(BDI_vec),
# Region-level outcome
GSCORR = as.vector(GSCORR_vec),
# Mappings
cat = as.integer(cat_idx),
subj_idx = as.integer(subj_idx)
)
stan_data
rstan_options(auto_write = TRUE)
Sys.setenv(LOCAL_CPPFLAGS = '-march=native')
fit <- stan(
file = "~/projects/MultGroup_WC/REPLICATION/R/Hierarahical_UTF8.stan",  # your Stan model
data = stan_data,
iter = 2000, chains = 4, cores = 4,
seed = 123
)
summary(fit)
print(fit, pars = c("alpha0","alphaA","alphaS","alphaM","alphaG",
"gamma0","betaA","betaS","betaM","betaG","betaB",
"sigma_BDI","sigma_GSCORR",
"sigma_bdiRE","sigma_gsRE"))  # main parameters
n_regions <- 5
n_obs     <- n_subj * n_regions
#=#=#=#=#=#=#=#=#=#=#=#=#=#=#=#=#=#=
library(dagitty)
library(rstan)
alpha0_true  <- 15    # global intercept for BDI
alphaA_true  <-  0.1  # slope age -> BDI
alphaS_true  <-  1.5  # slope sex -> BDI
alphaM_true  <- -0.2  # slope med -> BDI
alphaG_true  <-  2.0  # slope group -> BDI
sigma_BDI_true <- 2.0 # residual SD for BDI
# Random effects: correlated intercept & slope for Group
Sigma_bdi <- matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
# 2x2 covariance matrix for (int_offset, slope_offset_G)
# => correlation ~ 0.5, SDs ~ c(1,1)
n_subj   <- 20
Age_vec  <- rnorm(n_subj, 45, 10)             # Age ~ Normal(45,10)
Sex_vec  <- rbinom(n_subj, 1, 0.4)            # Sex ~ Bernoulli(p=0.4)
Med_vec  <- rnorm(n_subj, 0, 1)               # Medication ~ Normal(0,1)
Group_vec<- rbinom(n_subj, 1, 0.5)            # Group ~ Bernoulli(p=0.5)
library(MASS)
# Each subject j has [int_j, slopeG_j] ~ MVN( [0,0], Sigma_bdi )
RE_bdi <- mvrnorm(n_subj, mu = c(0,0), Sigma = Sigma_bdi)
int_bdi_offset   <- RE_bdi[,1]
slope_bdi_offset <- RE_bdi[,2]
BDI_vec <- numeric(n_subj)
for(j in 1:n_subj) {
mu_bdi_j <- (alpha0_true + int_bdi_offset[j]) +
alphaA_true * Age_vec[j] +
alphaS_true * Sex_vec[j] +
alphaM_true * Med_vec[j] +
(alphaG_true + slope_bdi_offset[j]) * Group_vec[j]
BDI_vec[j] <- rnorm(1, mu_bdi_j, sigma_BDI_true)
}
BDI_vec
K <- 5
Sigma_gs <- matrix(c(1.0, 0.3,
0.3, 1.0), 2,2)
# correlation ~ 0.3
# True fixed effects for GSCORR
gamma0_true <-  0.5
betaA_true  <-  0.02
betaS_true  <-  0.5
betaM_true  <- -0.1
betaG_true  <-  0.3
betaB_true  <-  1.0  # slope for BDI
sigma_GSCORR_true <- 1.5
# Category-level random intercept & slope for BDI
RE_gs_cat <- mvrnorm(K, mu=c(0,0), Sigma=Sigma_gs)
cat_int_offset  <- RE_gs_cat[,1]
cat_slope_offset<- RE_gs_cat[,2]
n_regions <- K * 20
n_obs     <- n_subj * n_regions
subj_idx  <- rep(1:n_subj, each=n_regions)
subj_idx
cat_idx   <- sample.int(K, size=n_obs, replace=TRUE)  # e.g., 1..K uniformly
cat_idx
GSCORR_vec <- numeric(n_obs)
for(i in 1:n_obs) {
s <- subj_idx[i]      # subject for obs i
k <- cat_idx[i]       # category for obs i
mu_gs_i <- (gamma0_true + cat_int_offset[k]) +
betaA_true * Age_vec[s] +
betaS_true * Sex_vec[s] +
betaM_true * Med_vec[s] +
betaG_true * Group_vec[s] +
(betaB_true + cat_slope_offset[k]) * BDI_vec[s]
GSCORR_vec[i] <- rnorm(1, mu_gs_i, sigma_GSCORR_true)
}
stan_data <- list(
# Indices & sizes
n = n_obs,
n_subj = n_subj,
n_regions = n_regions,
K = K,
# Subject-level covariates
S = as.integer(Sex_vec),
A = as.vector(Age_vec),
G = as.vector(Group_vec),
M = as.vector(Med_vec),
# Subject-level outcome
BDI = as.vector(BDI_vec),
# Region-level outcome
GSCORR = as.vector(GSCORR_vec),
# Mappings
cat = as.integer(cat_idx),
subj_idx = as.integer(subj_idx)
)
fit <- stan(
file = "~/projects/MultGroup_WC/REPLICATION/R/Hierarahical_UTF8.stan",  # your Stan model
data = stan_data,
iter = 2000, chains = 4, cores = 4,
seed = 123
)
print(fit, pars = c("alpha0","alphaA","alphaS","alphaM","alphaG",
"gamma0","betaA","betaS","betaM","betaG","betaB",
"sigma_BDI","sigma_GSCORR",
"sigma_bdiRE","sigma_gsRE"))  # main parameters
rstan:::rstudio_stanc("gscorr_hier.stan")
rstan:::rstudio_stanc("gscorr_hier.stan")
library(magick)
library(tidyverse)
img1 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part1.jpg") %>% image_resize("x300")
img2 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part2.jpg") %>% image_resize("x300")
img3 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part3.jpg") %>% image_resize("x300")
img4 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part4.jpg") %>% image_resize("x300")
bar <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/scale.png") %>% image_resize("170x")
filler <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/black_rectangle.png")
img12 <- image_append(c(img1, img2, bar), stack = FALSE)
img34 <- image_append(c(img3, img4, filler), stack = FALSE)
merged_img <- image_append(c(img12, img34), stack = TRUE)
merged_img
image_append(bar,filler,stack = TRUE)
image_append(c(bar,filler),stack = TRUE)
library(magick)
library(tidyverse)
img1 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part1.jpg") %>% image_resize("x300")
img2 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part2.jpg") %>% image_resize("x300")
img3 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part3.jpg") %>% image_resize("x300")
img4 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part4.jpg") %>% image_resize("x300")
bar <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/scale_page.png") %>% image_resize("300x")
filler <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/black_rectangle.png")
img12 <- image_append(c(img1, img2, bar), stack = FALSE)
img34 <- image_append(c(img3, img4, filler), stack = FALSE)
merged_img <- image_append(c(img12, img34), stack = TRUE)
merged_img
bar
img12
img34
img1
img2
bar
library(magick)
library(tidyverse)
img1 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part1.jpg") %>% image_resize("x300")
img2 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part2.jpg") %>% image_resize("x300")
img3 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part3.jpg") %>% image_resize("x300")
img4 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part4.jpg") %>% image_resize("x300")
bar <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/scale_page.png") %>% image_resize("600x")
filler <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/black_rectangle.png")
img12 <- image_append(c(img1, img2), stack = FALSE)
img34 <- image_append(c(img3, img4), stack = FALSE)
img1234 <- image_append(c(img12, img34), stack = TRUE)
img1234
merged_img <- image_append(c(img1234,bar),stack = FALSE)
merged_img
img1234
bar
img1 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part1.jpg") %>% image_resize("x300")
img2 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part2.jpg") %>% image_resize("x300")
img3 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part3.jpg") %>% image_resize("x300")
img4 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part4.jpg") %>% image_resize("x300")
bar <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/scale_page.png") %>% image_resize("x600")
filler <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/black_rectangle.png")
img12 <- image_append(c(img1, img2), stack = FALSE)
img34 <- image_append(c(img3, img4), stack = FALSE)
img1234 <- image_append(c(img12, img34), stack = TRUE)
merged_img <- image_append(c(img1234,bar),stack = FALSE)
merged_img
library(magick)
library(tidyverse)
img1 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part1.jpg") %>% image_resize("x300")
img2 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part2.jpg") %>% image_resize("x300")
img3 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part3.jpg") %>% image_resize("x300")
img4 <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/part4.jpg") %>% image_resize("x300")
bar <- image_read("/Users/kaankeskin/projects/AchStat/cerebellum/scale_page.png") %>% image_resize("x600")
img12 <- image_append(c(img1, img2), stack = FALSE)
img34 <- image_append(c(img3, img4), stack = FALSE)
img1234 <- image_append(c(img12, img34), stack = TRUE)
merged_img <- image_append(c(img1234,bar),stack = FALSE)
image_write(merged_img, path = "/Users/kaankeskin/projects/AchStat/cerebellum/merged_image.png", format = "png")
# Library
library(ggplot2)
library(dplyr)
library(rstatix)
library(ggpubr)
# Importing data
bpb_core <- scan("/Volumes/HD-B1/BPB_proc/bpb_subjects_core.1D",what = numeric())
bpb_periphery <- scan("/Volumes/HD-B1/BPB_proc/bpb_subjects_periphery.1D",what = numeric())
mdd_core <- scan("/Volumes/HD-B1/extra-proc/mdd_subjects_core.1D",what = numeric())
mdd_periphery <- scan("/Volumes/HD-B1/extra-proc/mdd_subjects_periphery.1D",what = numeric())
hc_core <- scan("/Volumes/HD-B1/extra-proc/hc_subjects_core.1D",what = numeric())
hc_periphery <- scan("/Volumes/HD-B1/extra-proc/hc_subjects_periphery.1D",what = numeric())
hc_periphery <- scan("/Volumes/HD-B1/extra-proc/hc_subjects_periphery.1D",what = numeric())
# Importing data
bpb_core <- scan("/Volumes/HD-B1/BPB_proc/bpb_subjects_core.1D",what = numeric())
bpb_periphery <- scan("/Volumes/HD-B1/BPB_proc/bpb_subjects_periphery.1D",what = numeric())
mdd_core <- scan("/Volumes/HD-B1/extra-proc/mdd_subjects_core.1D",what = numeric())
mdd_periphery <- scan("/Volumes/HD-B1/extra-proc/mdd_subjects_periphery.1D",what = numeric())
hc_core <- scan("/Volumes/HD-B1/extra-proc/hc_subjects_core.1D",what = numeric())
hc_periphery <- scan("/Volumes/HD-B1/extra-proc/hc_subjects_periphery.1D",what = numeric())
bpb_len <- length(bpb_core)
mdd_len <- length(mdd_core)
hc_len <- length(hc_core)
# Tidying data to long format
data_long <- data.frame(
value = c(bpb_core,bpb_periphery,mdd_core,mdd_periphery,hc_core,hc_periphery),
topo = c(rep(c("C","P"),each=bpb_len), rep(c("C","P"),each=mdd_len), rep(c("C","P"),each=hc_len)),
group = c(rep("BP", bpb_len*2), rep("MDD",mdd_len*2), rep("HC", hc_len*2))
)
# Filter data for topo C and P
data_filtered <- data_long %>% filter(topo %in% c("C", "P"))
# Compute max values per topo for y-position of significance annotations
max_values <- data_filtered %>%
group_by(topo) %>%
summarise(y_max = max(value, na.rm = TRUE))  # Ensures itâ€™s numeric
# Perform t-test for each topo separately
stat_test <- data_filtered %>%
group_by(topo) %>%
t_test(value ~ group,var.equal = FALSE ) %>% # Welch's Method
adjust_pvalue(method = "bonferroni") %>%
add_significance()
stat_test
stat_test <- stat_test %>%
left_join(max_values, by = "topo") %>%
mutate(y.position = y_max + 0.05 + (row_number() - 1) * 0.05)  # Adjust incrementally
# Print test results to check correctness
print(stat_test)
# Violin plot with statistical annotations
p <- ggplot(data_filtered, aes(x = group, y = value, fill = group)) +
geom_violin(trim = FALSE, alpha = 0.6) +
geom_jitter(width = 0.1, alpha = 0.5) +
facet_wrap(~topo) +
stat_pvalue_manual(
stat_test,
label = "p.adj.signif",
inherit.aes = FALSE,
mapping = aes(xmin = group1, xmax = group2, y.position = y.position)
) +
theme_bw() +  # Ensures a white background
labs(title = "", x = "Group", y = "Value") +
theme(legend.position = "none")
p
install.packages("haven")  # Install if not already installed
library(haven)
dat <- read_sav("/Users/kaankeskin/Downloads/alzheimer.sav")
dat
View(dat)
library(reticulate)
library(NeuroMyelFC)
source_folder <- "/Volumes/HD-B1/BPB_proc"
bpb_list <- list.dirs(path = source_folder, recursive = FALSE)
# Function to read a specific .1D file in each directory
read_1D <- function(dir_path, filename) {
# Construct full file path
file_path <- file.path(paste0(dir_path,"/Zscore_data_bandpass"), "glasser_all.1D")
# Check if the file exists
if (!file.exists(file_path)) {
return(NULL)  # Return NULL if the file is missing
}
# Read the .1D file (as numeric data)
file_data <- scan(file_path, what = numeric(), quiet = TRUE)
return(file_data)
}
# Apply the function to each folder
bpb_results <- lapply(bpb_list, function(dir) read_1D(dir, target_file))
# Assign folder names as list names
names(bpb_results) <- basename(bpb_list)
# Convert list to matrix and compute column-wise mean
num_matrix <- do.call(rbind, bpb_results)
bpb_av <- colMeans(num_matrix)  # Compute element-wise mean
head(bpb_av)
source_folder <- "/Volumes/HD-B1/BIDS/derivatives/afni"
hc_list <- paste0("/Volumes/HD-B1/BIDS/derivatives/afni/sub-", 34:70, ".results")
hc_list <- c(hc_list, paste0("/Volumes/HD-B1/extra-proc/sub-", 86:99, ".results"))
# Apply the function to each folder
hc_results <- lapply(hc_list, function(dir) read_1D(dir, target_file))
# Convert list to matrix and compute column-wise mean
num_matrix <- do.call(rbind, hc_results)
hc_av <- colMeans(num_matrix)  # Compute element-wise mean
head(hc_av)
bpb_av <- colMeans(num_matrix)  # Compute element-wise mean
csv_path <- "/Users/kaankeskin/projects/MultGroup_WC/REPLICATION/data/bp_array.csv"
write.csv(bpb_av, csv_path, row.names=FALSE)
csv_path <- "/Users/kaankeskin/projects/MultGroup_WC/REPLICATION/data/hc_array.csv"
write.csv(hc_av, csv_path, row.names = FALSE)
# Convert list to matrix and compute column-wise mean
num_matrix <- do.call(rbind, bpb_results)
bpb_av <- colMeans(num_matrix)  # Compute element-wise mean
bpb_av
bpb_av <- colMeans(num_matrix)  # Compute element-wise mean
csv_path <- "/Users/kaankeskin/projects/MultGroup_WC/REPLICATION/data/bp_array.csv"
write.csv(bpb_av, csv_path, row.names=FALSE)
# Apply the function to each folder
hc_results <- lapply(hc_list, function(dir) read_1D(dir, target_file))
# Convert list to matrix and compute column-wise mean
num_matrix <- do.call(rbind, hc_results)
hc_av <- colMeans(num_matrix)  # Compute element-wise mean
csv_path <- "/Users/kaankeskin/projects/MultGroup_WC/REPLICATION/data/hc_array.csv"
write.csv(hc_av, csv_path, row.names = FALSE)
library(R.matlab); library(tidyverse)
#
dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/pe_array.mat")
#
dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/pe_array.mat")
dat[:,1:60]
dat[,1:60]
dat$merged.matrix(,1:60)
dat$merged.matrix[,1:60]
pe_mat <- dat$merged.matrix[,1:60]
group <- ifelse(pe_mat[,61],"sz","hc")
ifelse(dat$merged.matrix[,61],"sz","hc")
group <- ifelse(dat$merged.matrix[,61],"sz","hc")
task <- repmat(c(1,2,3),each=20)
task <- rep(c(1,2,3),each=20)
task
str(pe_mat)
str(group)
str(task)
library(reshape2)
# Convert pe_mat to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- 1:52  # Assign a unique ID to each subject
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Add Group information
long_pe$Group <- rep(group, times = ncol(pe_mat))  # Repeat for each column
# Add Task information
long_pe$Task <- rep(task, each = nrow(pe_mat))  # Assign the correct task for each trial
# Check structure
str(long_pe)
head(long_pe)
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary(anova_model)
library(lme4)
anova_model <- lmer(PE ~ Group * Task + (1|Subject), data = long_pe)
anova(anova_model)
# Loading libraries
library(R.matlab); library(tidyverse)
dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/normalized_pe_array.mat")
pe_mat <- dat$merged.matrix[,1:60]
group <- ifelse(dat$merged.matrix[,61],"sz","hc")
task <- rep(c(1,2,3),each=20)
library(reshape2)
# Convert pe_mat to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- 1:52  # Assign a unique ID to each subject
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Add Group information
long_pe$Group <- rep(group, times = ncol(pe_mat))  # Repeat for each column
# Add Task information
long_pe$Task <- rep(task, each = nrow(pe_mat))  # Assign the correct task for each trial
# Check structure
str(long_pe)
head(long_pe)
library(lme4)
anova_model <- lmer(PE ~ Group * Task + (1|Subject), data = long_pe)
anova(anova_model)
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary(anova_model)
pe_mat
library(R.matlab); library(tidyverse)
#
dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/pe_array.mat")
#dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/normalized_pe_array.mat")
pe_mat <- dat$merged.matrix[,1:60]
group <- ifelse(dat$merged.matrix[,61],"sz","hc")
task <- rep(c(1,2,3),each=20)
library(reshape2)
# Convert pe_mat to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- 1:52  # Assign a unique ID to each subject
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Add Group information
long_pe$Group <- rep(group, times = ncol(pe_mat))  # Repeat for each column
# Add Task information
long_pe$Task <- rep(task, each = nrow(pe_mat))  # Assign the correct task for each trial
# Check structure
str(long_pe)
head(long_pe)
library(lme4)
anova_model <- lmer(PE ~ Group * Task + (1|Subject), data = long_pe)
anova(anova_model)
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary(anova_model)
dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/pe_array.mat")
#dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/normalized_pe_array.mat")
pe_mat <- dat$merged.matrix[,1:60]
group <- factor(ifelse(dat$merged.matrix[,61],"sz","hc"))
task <- factor(rep(c(1,2,3),each=20))
library(reshape2)
# Convert pe_mat to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- 1:52  # Assign a unique ID to each subject
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Add Group information
long_pe$Group <- rep(group, times = ncol(pe_mat))  # Repeat for each column
# Add Task information
long_pe$Task <- rep(task, each = nrow(pe_mat))  # Assign the correct task for each trial
# Check structure
str(long_pe)
plot(density(long_pe$PE))
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary(anova_model)
library(emmeans)
# Fit mixed ANOVA model again
library(lme4)
anova_model <- lmer(PE ~ Group * Task + (1|Subject), data = long_pe)
# Estimated Marginal Means (EMMs) for Task within each Group
task_emmeans <- emmeans(anova_model, pairwise ~ Task | Group, adjust = "bonferroni")
# Show pairwise comparisons
task_emmeans
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
summary(anova_model)
# Estimated Marginal Means (EMMs) for Task within each Group
task_emmeans <- emmeans(anova_model, pairwise ~ Task | Group, adjust = "bonferroni")
# Show pairwise comparisons
task_emmeans
task_emmeans <- emmeans(anova_model, pairwise ~ Group | Task, adjust = "bonferroni")
anova_model <- aov(PE ~ Group * Task + Error(Subject/Task), data = long_pe)
task_emmeans <- emmeans(anova_model, pairwise ~ Group , adjust = "bonferroni")
# Show pairwise comparisons
task_emmeans
summary(anova_model)
library(emmeans)
# Estimated Marginal Means (EMMs) for Task within each Group
task_emmeans <- emmeans(anova_model, pairwise ~ Task | Group, adjust = "bonferroni")
# Show pairwise comparisons
task_emmeans
library(R.matlab); library(tidyverse)
setwd("/Users/kaankeskin/projects/sch_pe/")
#
dat <- readMat("./data/processed/pe_array.mat")
subj_table <- read.csv("./data/raw/subjects_list.csv")
#dat <- readMat("/Users/kaankeskin/projects/sch_pe/data/processed/normalized_pe_array.mat")
pe_mat <- dat$merged.matrix[,1:60]
group <- factor(ifelse(dat$merged.matrix[,61],"sz","hc"))
task <- factor(rep(c(1,2,3),each=20))
sex <- factor(ifelse(subj_table$sex,"M","F"))
library(reshape2)
# Convert pe_mat to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- 1:52  # Assign a unique ID to each subject
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Add Group information
long_pe$Group <- rep(group, times = ncol(pe_mat))  # Repeat for each column
# Add Task information
long_pe$Task <- rep(task, each = nrow(pe_mat))  # Assign the correct task for each trial
# Add Sex information
long_pe$Sex <- rep(sex, times = ncol(pe_mat))  # Assign the correct task for each trial
# Check structure
str(long_pe)
head(long_pe)
plot(density(long_pe))
plot(density(long_pe$PE))
long_pe %>% group_by(Group) %>% rstatix::t_test(PE ~ Task)
long_pe %>% group_by(Task) %>% rstatix::t_test(PE ~ Group)
