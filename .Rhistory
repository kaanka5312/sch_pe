print(n=59)
View(consistency_check)
key %>% filter(BCNatlas=="150")
key %>% filter(BCNatlas=="184")
key %>% filter(BCNatlas=="222")
key %>% filter(BCNatlas=="500")
library(ggseg)
library(ggseg) ; library(ggsegGlasser) ; library(rethinking); library(tidyverse)
someData <- tibble(
region = rep(c("transverse temporal", "insula",
"precentral","superior parietal"), 2),
p = sample(seq(0,.5,.001), 8),
groups = c(rep("g1", 4), rep("g2", 4))
)
someData %>%
group_by(groups) %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = p)) +
facet_wrap(~groups)
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R", "R")
)
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R")
)
p1 <- group12 %>%
group_by(hemisphere)  %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = t)) +
ggtitle("Iyi Yanit > Kotu Yanit")+
mytheme +
scale_fill_gradient(low = "gray", high="black", na.value = "white",limits=c(2.2,4.5))+
facet_wrap(~hemisphere)
mytheme <- theme_classic()+
theme(legend.position = "bottom",
legend.text = element_text(size = 10),
axis.line = element_blank(),
axis.ticks.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.y = element_blank(),
axis.text.y = element_blank(),
plot.title = element_text(size = 20, face = "bold", hjust = 0.5))
group12 <- tibble(
region = c("superior frontal", "superior frontal","pars opercularis", "pars opercularis","pars triangularis", "precentral", "precentral", "paracentral", "superior temporal"),
t = c(2.719,2.422,2.394,2.626,2.298,4.413,2.674,2.834,2.907),
hemisphere = c("R", "L", "R", "L","R", "R", "L", "R", "R")
)
p1 <- group12 %>%
group_by(hemisphere)  %>%
ggplot() +
geom_brain(atlas = dk,
position = position_brain(hemi ~ side),
aes(fill = t)) +
ggtitle("Iyi Yanit > Kotu Yanit")+
mytheme +
scale_fill_gradient(low = "gray", high="black", na.value = "white",limits=c(2.2,4.5))+
facet_wrap(~hemisphere)
p1
ggarrange(p1,ncol = 1, common.legend = TRUE
)%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
library(ggpubr)
ggarrange(p1,ncol = 1, common.legend = TRUE
)%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
p1%>%ggpubr::ggexport(filename = "/Users/kaank/OneDrive/Belgeler/elif_figure2.png",width = 800,height = 400,res=150)
key %>% filter(BCNatlas=="120")
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
# source_path = "C:/Users/kaan/Documents/MultGroup_WC"
source_path = "C:/Users/kaank/OneDrive/Belgeler/GitHub/MultGroup_WC"
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
library(tidyverse)
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
#### Testing if regions are belonged to core or periphery ####
transmodal <- ifelse(NeuroMyelFC::trans_nonself | NeuroMyelFC::trans_self,1,0)
key <- key %>%
mutate(Category = transmodal)
# Check consistency for each BCNatlas group
consistency_check <- key %>%
group_by(BCNatlas) %>%
summarize(
Is_Consistent = if (n_distinct(Category) == 1) "YES" else "NO" # TRUE if all categories are the same
)
# View results
print(consistency_check)
key %>% filter(BCNatlas=="120")
key %>% filter(BCNatlas=="121")
key
#### Organizing the connectivity matrix
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity.csv",row.names = 1)
#### Organizing the connectivity matrix
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
header = TRUE,
row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
dat
dat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
subject_list
key
averaged_rois_list
Cmat <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_dti_connectivity",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
Cmat
# Normalize by the maximum value
Cmat_max <- Cmat / max(Cmat)
# Check the normalized matrix
print(Cmat_max)
# View results for the first vector
print(averaged_rois_list[[1]])
read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
### ROI normalization ###
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roi_list
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roilist
roilist
roilist
roilist[1]
roilist[1,1]
ro
roilist[612:617,]
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = TRUE,
row.names = 1)
roilist
roilist <- read.csv("C:/Users/kaank/Downloads/dti_connectivity/sub-2_roivoxels",
sep="\t",
check.names = FALSE,
header = FALSE,
row.names = 1)
roilist
roilist[612,]
roilist[1,]
roilist[612,]
# List of regions to select
regions <- c("R. transverse temporal gyrus", "R. superior temporal gyrus (gm)")
# Filter rows based on the region names in column V2
selected_voxels <- roilist[roilist$V2 %in% regions, "V3"]
# Display the result
print(selected_voxels)
# List of regions to select
regions <- c("R. transverse temporal gyrus (gm)", "R. superior temporal gyrus (gm)")
# Filter rows based on the region names in column V2
selected_voxels <- roilist[roilist$V2 %in% regions, "V3"]
# Display the result
print(selected_voxels)
key
averaged_rois_list
print(averaged_rois_list[[1]],n=59)
library(readxl)
key <- read_xlsx(paste0(source_path,"/DATA/myDataParcels.xlsx"))
HC <- read.table(file = paste0(source_path,"/DATA/HC.csv"),sep=",")
BP <- read.table(file = paste0(source_path,"/DATA/BP.csv"),sep=",")
MDD <- read.table(file = paste0(source_path,"/DATA/MDD.csv"),sep=",")
Manic <- read.table(file = paste0(source_path,"/DATA/Manic.csv"),sep=",")
subject_list <- list(HC, BP, MDD, Manic)
roi_list<- lapply(subject_list, colMeans)
# Add ROI values to the `key` tibble
average_by_bcnatlas <- function(roi_vector, key) {
key %>%
mutate(ROI_value = roi_vector) %>%        # Add the ROI vector as a column
group_by(BCNatlas) %>%                   # Group by BCNatlas
summarize(Average_ROI = mean(ROI_value, na.rm = TRUE)) # Calculate the average
}
# Apply the function to each vector in the list
averaged_rois_list <- lapply(roi_list, average_by_bcnatlas, key = key)
# View results for the first vector
print(averaged_rois_list[[1]])
print(averaged_rois_list[[1]],n=58)
library(ggseg)
library(ggsegGlasser)
glasser
ggplot() + geom_brain()
ggsegGlasser::glasser
ggplot() + geom_brain( atlas = glasser)
ggplot() + geom_brain( atlas = glasser,
show.legend = FALSE,
position=position_brain(position= hemi ~ side),
mapping = aes(fill = "6d") )
ggplot() + geom_brain( atlas = glasser)
library(reticulate)
library(NeuroMyelFC)
# Set the Python path (adjust the path to your Python executable)
# Use the specific Python executable
#use_python("C:/Users/kaank/AppData/Local/Programs/Python/Python311/python.exe")
source_folder = "C:/Users/kaank/OneDrive/Belgeler/GitHub/MultGroup_WC/"
Sys.setenv(RETICULATE_PYTHON = "C:/Users/kaank/miniconda3/envs/multgroup/python.exe")
library(reticulate)
np <- import("numpy")
array <- np$load(paste(source_folder,"group_freekgl3.npy",sep=""),allow_pickle = TRUE)
merged_matrices <- list()
# Get the names from the list
array_names <- names(array[[1]])
# Loop through each name in the list
for (name in array_names) {
# Initialize empty matrices to store the merged results for each name
merged_acw_static_firing_matrix <- NULL
merged_acw_dynamic_firing_matrix <- NULL
merged_acw_static_fmri_matrix <- NULL
merged_gscorr_fmri_matrix <- NULL
merged_gscorr_firing_matrix <- NULL
# Loop through each element in the sublist
for (i in 1:length(array[[1]]$MDD)) {
merged_acw_static_firing_matrix <- rbind(merged_acw_static_firing_matrix, array[[1]][[name]][[i]]$acw_static_firing_matrix)
merged_acw_dynamic_firing_matrix <- rbind(merged_acw_dynamic_firing_matrix, array[[1]][[name]][[i]]$acw_dynamic_firing_matrix)
merged_acw_static_fmri_matrix <- rbind(merged_acw_static_fmri_matrix, array[[1]][[name]][[i]]$acw_static_fmri_matrix)
merged_gscorr_fmri_matrix <- rbind(merged_gscorr_fmri_matrix, array[[1]][[name]][[i]]$gscorr_fmri_matrix)
merged_gscorr_firing_matrix <- rbind(merged_gscorr_firing_matrix, array[[1]][[name]][[i]]$gscorr_firing_matrix)
}
# Save the merged matrices into the final list
merged_matrices[[name]] <- list(
merged_acw_static_firing_matrix = merged_acw_static_firing_matrix,
merged_acw_dynamic_firing_matrix = merged_acw_dynamic_firing_matrix,
merged_acw_static_fmri_matrix = merged_acw_static_fmri_matrix,
merged_gscorr_fmri_matrix = merged_gscorr_fmri_matrix,
merged_gscorr_firing_matrix = merged_gscorr_firing_matrix
)
}
gs_sim = list()
gs_sim$mdd = merged_matrices$MDD$merged_gscorr_fmri_matrix
gs_sim$hc = merged_matrices$HC$merged_gscorr_fmri_matrix
gs_sim$manic = merged_matrices$Manic$merged_gscorr_fmri_matrix
gs_sim$bp = merged_matrices$BP$merged_gscorr_fmri_matrix
uni <- NeuroMyelFC::uni_nonself | NeuroMyelFC::uni_self
trans <- NeuroMyelFC::trans_nonself | NeuroMyelFC::trans_self
gs_sim
dat <- lapply(gs_sim, function(mat){
masked_mat <- mat[, uni, drop = FALSE]  # Apply the mask to select columns
rowMeans(masked_mat)                    # Compute row-wise means for selected columns
})
sim_uni <- data.frame(
value=c(dat$hc,dat$mdd,dat$manic,dat$bp),
group = c(rep(c("hc","mdd","manic","bp"),each=90))
)
sim_uni
anova_result <- aov(value~group,data=sim_uni)
TukeyHSD(anova_result)
ggplot(sim_uni,aes( x = group, y = value)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Unimodal - Simulation")
dat <- lapply(gs_sim, function(mat){
masked_mat <- mat[, trans, drop = FALSE]  # Apply the mask to select columns
rowMeans(masked_mat)                    # Compute row-wise means for selected columns
})
sim_trans <- data.frame(
value=c(dat$hc,dat$mdd,dat$manic,dat$bp),
group = c(rep(c("hc","mdd","manic","bp"),each=90))
)
anova_result <- aov(value~group,data=sim_trans)
summary(anova_result) # report
anova_result <- aov(value~group,data=sim_trans)
TukeyHSD(anova_result)
summary(anova_result) # report
ggplot(sim_trans,aes( x = group, y = value)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Transmodal - Simulation")
sim_trans
dat <- lapply(gs_sim, function(mat){
masked_mat <- mat[, trans, drop = FALSE]  # Apply the mask to select columns
rowMeans(masked_mat)                    # Compute row-wise means for selected columns
})
sim_trans <- data.frame(
value=c(dat$hc,dat$mdd,dat$manic,dat$bp),
group = c(rep(c("hc","mdd","manic","bp"),each=90))
)
sim_trans
anova_result <- aov(value~group,data=sim_trans)
TukeyHSD(anova_result)
# Load the package
library(effectsize)
# Calculate eta squared
eta_squared(anova_result)
anova_result
TukeyHSD(anova_result)
summary(anova_result)
anova_result <- aov(value~group,data=sim_uni)
TukeyHSD(anova_result)
ggplot(sim_uni,aes( x = group, y = value)) +
geom_boxplot() +
theme_minimal()
array1<-c(39,33,14,32,10,50,47,30,34,50,37,42,47,44,50,33,46,28,46,29,49,33,54,31,37,35
)
array2 <- c(48,34,51,35,25,25,46,22,8,38,52,28,10,37,27,24)
kruskal.test(array1,array2)
shapiro.test(array1)
shapiro.test(array2)
t_test_result <- t.test(array1, array2, var.equal = FALSE)
print(t_test_result)
library(R.matlab); library(tidyverse); library(reshape2)
#setwd("/Users/kaankeskin/projects/sch_pe/")
# Microsoft
setwd("C:/Users/kaank/OneDrive/Belgeler/GitHub/sch_pe/")
#
dat <- list(readMat("./data/processed/normalized_pe_array2.mat"), # Cemre RW PE
readMat("./data/processed/x2_array.mat"), # HGF X2
readMat("./data/processed/x3_array.mat"), # HGF X3
readMat("./data/processed/x2_pe_array.mat"), # HGF low level PE
readMat("./data/processed/x3_pe_array.mat"), # HGF high level PE
readMat("./data/processed/alfa2_array.mat"), # learning rate level 2
readMat("./data/processed/alfa3_array.mat"), # learning rate level 3
readMat("./data/processed/rw_pe.mat") # RW model PE from TAPAS
)
subj_table <- read.csv("./data/raw/subjects_list.csv")
subj_table
library(R.matlab); library(tidyverse); library(reshape2)
#setwd("/Users/kaankeskin/projects/sch_pe/")
# Microsoft
setwd("C:/Users/kaank/OneDrive/Belgeler/GitHub/sch_pe/")
dat <- readxl::read_xlsx("./data/raw/DataElif.xlsx")
subj_table <- read.csv("./data/raw/subjects_list.csv")
df_sz <- subj_table[subj_table$group==1, ]
df_sz <- df_sz[order(df_sz$name),]
df_sz
dat
dat %>% filter(Dx==1)
dat %>% filter(Dx==1
) %>% arrange(NameSurname)
dat_sz <- dat %>% filter(Dx==1
) %>% arrange(NameSurname)
View(dat_sz)
View(df_sz)
dat <- readxl::read_xlsx("./data/raw/DataElif.xlsx")
dat <- readxl::read_xlsx("./data/raw/DataElif.xlsx")
dat
dat %>% filter(Dx==1
) %>% arrange(NameSurname
) %>% filter(!is.na(subj))
dat_sz <- dat %>% filter(Dx==1
) %>% arrange(NameSurname
) %>% filter(!is.na(subj))
convert_to_long <- function(dat, subj_table) {
# Extract PE matrix (first 60 columns) and group information (column 61)
pe_mat <- dat$merged.matrix[,1:60]
group <- factor(ifelse(dat$merged.matrix[,61], "sz", "hc"))
task <- factor(rep(1:3, each = 20))  # Assuming 3 tasks with 20 trials each
sex <- factor(ifelse(subj_table$sex, "M", "F"))
# Convert PE matrix to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- seq_len(nrow(pe_mat))  # Assign unique IDs to subjects
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Convert Trial variable to numeric
#long_pe$Trial <- as.numeric(gsub("V", "", long_pe$Trial))  # Remove "V" prefix if needed
# Add Group information (repeat for each trial)
long_pe$Group <- rep(group, each = ncol(pe_mat))
# Add Task information (repeat for each subject)
long_pe$Task <- rep(task, times = nrow(pe_mat))
# Add Sex information (repeat for each trial)
long_pe$Sex <- rep(sex, each = ncol(pe_mat))
return(long_pe)
}
pe_long <- convert_to_long(pe,subj_table = subj_table)
pe <- readMat("./data/processed/normalized_pe_array2.mat")
convert_to_long <- function(dat, subj_table) {
# Extract PE matrix (first 60 columns) and group information (column 61)
pe_mat <- dat$merged.matrix[,1:60]
group <- factor(ifelse(dat$merged.matrix[,61], "sz", "hc"))
task <- factor(rep(1:3, each = 20))  # Assuming 3 tasks with 20 trials each
sex <- factor(ifelse(subj_table$sex, "M", "F"))
# Convert PE matrix to a data frame
pe_df <- as.data.frame(pe_mat)
# Add Subject IDs
pe_df$Subject <- seq_len(nrow(pe_mat))  # Assign unique IDs to subjects
# Reshape to long format
long_pe <- melt(pe_df, id.vars = "Subject", variable.name = "Trial", value.name = "PE")
# Convert Trial variable to numeric
#long_pe$Trial <- as.numeric(gsub("V", "", long_pe$Trial))  # Remove "V" prefix if needed
# Add Group information (repeat for each trial)
long_pe$Group <- rep(group, each = ncol(pe_mat))
# Add Task information (repeat for each subject)
long_pe$Task <- rep(task, times = nrow(pe_mat))
# Add Sex information (repeat for each trial)
long_pe$Sex <- rep(sex, each = ncol(pe_mat))
return(long_pe)
}
pe_long <- convert_to_long(pe,subj_table = subj_table)
pe_long
pe_long %>%
filter(Subject %in% dat_sz$subj) %>%
group_by(Subject) %>%
summarise(rmse_PE = sqrt(mean(PE^2)))
dat_sz
rmse_summary <- pe_long %>%
filter(Subject %in% dat_sz$subj) %>%
group_by(Subject) %>%
summarise(rmse_PE = sqrt(mean(PE^2)))
dat_sz$rmse <- rmse_summary$rmse_PE
dat_sze
str(dat_sze)
str(dat_sz)
dat_sz$`PANSS-Positive`
dat_sz$SAPS
dat_sz$P1
dat_sz %>% filter(!is.na(`PANSS-Positive`))
dat_sz
dat_sz$`PANSS-Positive`
dat_sz %>% filter(!is.na(`PANSS-Positive`))
dat_sz %>% filter(!is.na(`PANSS-Positive`))
dat_sz %>% filter(is.na(`PANSS-Positive`))
dat_sz %>% filter(!is.na(`PANSS-Positive`))
dat_sz %>% filter(!is.na(`PANSS-Positive`)) %>% pull(PANSS-Positive)
dat_sz %>% filter(!is.na(`PANSS-Positive`)) %>% pull(`PANSS-Positive`)
dat_sz$`PANSS-Positive` <- as.numeric(dat_sz$`PANSS-Positive`)
dat_sz %>% filter(!is.na(`PANSS-Positive`)) %>% pull(`PANSS-Positive`)
dat_sz %>% filter(!is.na(`PANSS-Positive`))
dat_sz <- dat_sz %>% filter(!is.na(`PANSS-Positive`))
ordered(dat_sz$rmse,levels=c("7","8","9","10","11"))
ordered(dat_sz$rmse,levels=c(7,8,9,10,11))
ordered(dat_sz$`PANSS-Positive`,levels=c(7,8,9,10,11))
dat_sz$`PANSS-Positive` <- ordered(dat_sz$`PANSS-Positive`,levels=c(7,8,9,10,11))
dat_sz$`PANSS-Positive`
library(MASS)
model <- polr(PANSS_Positive ~ rmse, data = dat_sz, Hess = TRUE)
model <- polr(`PANSS_Positive` ~ rmse, data = dat_sz, Hess = TRUE)
library(MASS)
model <- polr(`PANSS_Positive` ~ rmse, data = dat_sz, Hess = TRUE)
dat_sz$`PANSS-Positive` <- ordered(dat_sz$`PANSS-Positive`,levels=c(7,8,9,10,11))
library(MASS)
model <- polr(PANSS_Positive ~ rmse, data = dat_sz, Hess = TRUE)
library(MASS)
model <- polr(PANSS-Positive ~ rmse, data = dat_sz, Hess = TRUE)
model <- polr(`PANSS-Positive` ~ rmse, data = dat_sz, Hess = TRUE)
summary(model)
